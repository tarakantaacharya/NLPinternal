{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQF1hfRpV+CQ2+56QRbZ0A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarakantaacharya/NLPinternal/blob/main/NLP_lab_internal_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the steps to install and explore the features of **NLTK** and **spaCy**, as well as how to download word clouds and corpora:\n",
        "\n",
        "### Step 1: Install NLTK and spaCy\n",
        "\n",
        "1. **Install NLTK**:\n",
        "   Run this command in your terminal or command prompt:\n",
        "   ```bash\n",
        "   pip install nltk\n",
        "   ```\n",
        "\n",
        "2. **Install spaCy**:\n",
        "   Run this command in your terminal or command prompt:\n",
        "   ```bash\n",
        "   pip install spacy\n",
        "   ```\n",
        "\n",
        "3. **Install a spaCy Language Model** (for example, the English model):\n",
        "   ```bash\n",
        "   python -m spacy download en_core_web_sm\n",
        "   ```\n",
        "\n",
        "### Step 2: Exploring NLTK\n",
        "\n",
        "1. **Import NLTK and Download Corpora**:\n",
        "   After installation, you can import NLTK and download necessary corpora.\n",
        "   \n",
        "   ```python\n",
        "   import nltk\n",
        "   nltk.download('punkt')  # For tokenization\n",
        "   nltk.download('stopwords')  # For stop words\n",
        "   nltk.download('wordnet')  # For lemmatization\n",
        "   nltk.download('movie_reviews')  # Example of a corpus\n",
        "   ```\n",
        "\n",
        "2. **Explore NLTK Features**:\n",
        "   - **Tokenization**:\n",
        "     ```python\n",
        "     from nltk.tokenize import word_tokenize\n",
        "     text = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "     tokens = word_tokenize(text)\n",
        "     print(tokens)\n",
        "     ```\n",
        "\n",
        "   - **Stopwords**:\n",
        "     ```python\n",
        "     from nltk.corpus import stopwords\n",
        "     stop_words = set(stopwords.words('english'))\n",
        "     print(stop_words)\n",
        "     ```\n",
        "\n",
        "   - **Lemmatization**:\n",
        "     ```python\n",
        "     from nltk.stem import WordNetLemmatizer\n",
        "     lemmatizer = WordNetLemmatizer()\n",
        "     print(lemmatizer.lemmatize(\"running\", pos='v'))  # Verb lemmatization\n",
        "     ```\n",
        "\n",
        "   - **Word Frequency Distribution**:\n",
        "     ```python\n",
        "     from nltk.probability import FreqDist\n",
        "     fdist = FreqDist(tokens)\n",
        "     fdist.plot()  # To plot word frequency distribution\n",
        "     ```\n",
        "\n",
        "### Step 3: Exploring spaCy\n",
        "\n",
        "1. **Import and Load spaCy Language Model**:\n",
        "   ```python\n",
        "   import spacy\n",
        "   nlp = spacy.load('en_core_web_sm')\n",
        "   ```\n",
        "\n",
        "2. **Explore spaCy Features**:\n",
        "   - **Tokenization**:\n",
        "     ```python\n",
        "     doc = nlp(\"spaCy is a powerful NLP library.\")\n",
        "     for token in doc:\n",
        "         print(token.text)\n",
        "     ```\n",
        "\n",
        "   - **Part-of-Speech Tagging**:\n",
        "     ```python\n",
        "     for token in doc:\n",
        "         print(token.text, token.pos_)\n",
        "     ```\n",
        "\n",
        "   - **Named Entity Recognition**:\n",
        "     ```python\n",
        "     for ent in doc.ents:\n",
        "         print(ent.text, ent.label_)\n",
        "     ```\n",
        "\n",
        "   - **Dependency Parsing**:\n",
        "     ```python\n",
        "     for token in doc:\n",
        "         print(token.text, token.dep_, token.head.text)\n",
        "     ```\n",
        "\n",
        "### Step 4: Installing and Generating Word Cloud\n",
        "\n",
        "1. **Install WordCloud**:\n",
        "   ```bash\n",
        "   pip install wordcloud\n",
        "   ```\n",
        "\n",
        "2. **Generate Word Cloud**:\n",
        "   ```python\n",
        "   from wordcloud import WordCloud\n",
        "   import matplotlib.pyplot as plt\n",
        "\n",
        "   # Sample text for the word cloud\n",
        "   text = \"Python is a great language for machine learning and natural language processing.\"\n",
        "\n",
        "   # Create the word cloud\n",
        "   wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "\n",
        "   # Display the word cloud\n",
        "   plt.imshow(wordcloud, interpolation='bilinear')\n",
        "   plt.axis('off')\n",
        "   plt.show()\n",
        "   ```\n",
        "\n",
        "### Step 5: Downloading Corpora in NLTK\n",
        "\n",
        "1. **List Available Corpora**:\n",
        "   ```python\n",
        "   nltk.corpus.reader.__all__\n",
        "   ```\n",
        "\n",
        "2. **Download Additional Corpora**:\n",
        "   You can download various corpora like `gutenberg`, `reuters`, `abc`, etc., by running:\n",
        "   ```python\n",
        "   nltk.download('gutenberg')\n",
        "   nltk.download('reuters')\n",
        "   ```\n",
        "\n",
        "By following these steps, you should be able to explore the basic functionalities of NLTK and spaCy, work with corpora, and generate a word cloud."
      ],
      "metadata": {
        "id": "KDOQCySwA72S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Week 2\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uWfxIEkfLhEB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W_di--fLfPH",
        "outputId": "85a27bf2-a2a6-4149-a40a-f8da20ad19b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraphs:\n",
            " ['Natural Language Processing (NLP) helps computers understand human language.', 'Tokenization breaks text into words and sentences for analysis.']\n",
            "Paragraph 1: Natural Language Processing (NLP) helps computers understand human language.\n",
            "\n",
            "Paragraph 2: Tokenization breaks text into words and sentences for analysis.\n",
            "\n",
            "Sentences:\n",
            " ['Natural Language Processing (NLP) helps computers understand human language.', 'Tokenization breaks text into words and sentences for analysis.']\n",
            "Sentence 1: Natural Language Processing (NLP) helps computers understand human language.\n",
            "Sentence 2: Tokenization breaks text into words and sentences for analysis.\n",
            "\n",
            "Words:\n",
            " ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'helps', 'computers', 'understand', 'human', 'language', '.', 'Tokenization', 'breaks', 'text', 'into', 'words', 'and', 'sentences', 'for', 'analysis', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')  # Required for tokenizers\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Sample text with multiple paragraphs\n",
        "# text=input(\"enter text:\\n\")\n",
        "text = '''Natural Language Processing (NLP) helps computers understand human language.\n",
        "\n",
        "Tokenization breaks text into words and sentences for analysis.'''\n",
        "\n",
        "# Word Tokenizer\n",
        "def word_tokenizer(text):\n",
        "    words = word_tokenize(text)\n",
        "    return words\n",
        "\n",
        "# Sentence Tokenizer\n",
        "def sentence_tokenizer(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    return sentences\n",
        "\n",
        "# Paragraph Tokenizer\n",
        "def paragraph_tokenizer(text):\n",
        "    paragraphs = text.split(\"\\n\\n\")  # Splitting based on double newline characters\n",
        "    return paragraphs\n",
        "\n",
        "# Using the tokenizers\n",
        "# print(\"Original Text:\")\n",
        "# print(text, \"\\n\")\n",
        "\n",
        "# Paragraph Tokenization\n",
        "paragraphs = paragraph_tokenizer(text)\n",
        "print(\"Paragraphs:\\n\",paragraphs)\n",
        "for i, para in enumerate(paragraphs, 1):\n",
        "    print(f\"Paragraph {i}: {para}\\n\")\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentences = sentence_tokenizer(text)\n",
        "print(\"Sentences:\\n\",sentences)\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"Sentence {i}: {sentence}\")\n",
        "\n",
        "# Word Tokenization\n",
        "words = word_tokenizer(text)\n",
        "print(\"\\nWords:\\n\",words)\n",
        "# print(words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('brown')  # Download Brown corpus if not already downloaded\n",
        "\n",
        "# Select a corpus (example: Brown corpus)\n",
        "corpus_words = brown.words()\n",
        "\n",
        "# Calculate total words and unique words\n",
        "total_words = len(corpus_words)  # Total number of words\n",
        "distinct_words = len(set(corpus_words))  # Total unique words (using set)\n",
        "\n",
        "# Print results\n",
        "print(f\"Total words in the corpus: {total_words}\")\n",
        "print(f\"Number of distinct words in the corpus: {distinct_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQamWzRmLnZV",
        "outputId": "cc72be49-f762-4a62-d87a-410e68d8bff9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words in the corpus: 1161192\n",
            "Number of distinct words in the corpus: 56057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Week3"
      ],
      "metadata": {
        "id": "PVW76C-qMs7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def text_preprocessing(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize into words\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "def generate_ngrams(text, n):\n",
        "    tokens = text_preprocessing(text)\n",
        "    # Generate n-grams\n",
        "    n_grams = list(ngrams(tokens, n))\n",
        "    # Count frequencies\n",
        "    ngram_freq = Counter(n_grams)\n",
        "    return n_grams, ngram_freq\n",
        "\n",
        "# Example text\n",
        "text = \"Natural language processing is an exciting field of study.\"\n",
        "\n",
        "# Generate unigrams\n",
        "print(\"=== UNIGRAMS ===\")\n",
        "unigrams, unigram_freq = generate_ngrams(text, 1)\n",
        "print(\"\\nFirst 10 unigrams:\", unigrams[:10])\n",
        "print(\"\\nTop 10 most common unigrams:\")\n",
        "for gram, count in unigram_freq.most_common(10):\n",
        "    print(f\"{gram}: {count}\")\n",
        "\n",
        "# Generate bigrams\n",
        "print(\"\\n=== BIGRAMS ===\")\n",
        "bigrams, bigram_freq = generate_ngrams(text, 2)\n",
        "print(\"\\nFirst 10 bigrams:\", bigrams[:10])\n",
        "print(\"\\nTop 10 most common bigrams:\")\n",
        "for gram, count in bigram_freq.most_common(10):\n",
        "    print(f\"{gram}: {count}\")\n",
        "\n",
        "# Generate trigrams\n",
        "print(\"\\n=== TRIGRAMS ===\")\n",
        "trigrams, trigram_freq = generate_ngrams(text, 3)\n",
        "print(\"\\nFirst 10 trigrams:\", trigrams[:10])\n",
        "print(\"\\nTop 10 most common trigrams:\")\n",
        "for gram, count in trigram_freq.most_common(10):\n",
        "    print(f\"{gram}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkGFTPGiMsnR",
        "outputId": "549792e0-0a3f-4bce-f3f7-595a45debc32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== UNIGRAMS ===\n",
            "\n",
            "First 10 unigrams: [('natural',), ('language',), ('processing',), ('is',), ('an',), ('exciting',), ('field',), ('of',), ('study',), ('.',)]\n",
            "\n",
            "Top 10 most common unigrams:\n",
            "('natural',): 1\n",
            "('language',): 1\n",
            "('processing',): 1\n",
            "('is',): 1\n",
            "('an',): 1\n",
            "('exciting',): 1\n",
            "('field',): 1\n",
            "('of',): 1\n",
            "('study',): 1\n",
            "('.',): 1\n",
            "\n",
            "=== BIGRAMS ===\n",
            "\n",
            "First 10 bigrams: [('natural', 'language'), ('language', 'processing'), ('processing', 'is'), ('is', 'an'), ('an', 'exciting'), ('exciting', 'field'), ('field', 'of'), ('of', 'study'), ('study', '.')]\n",
            "\n",
            "Top 10 most common bigrams:\n",
            "('natural', 'language'): 1\n",
            "('language', 'processing'): 1\n",
            "('processing', 'is'): 1\n",
            "('is', 'an'): 1\n",
            "('an', 'exciting'): 1\n",
            "('exciting', 'field'): 1\n",
            "('field', 'of'): 1\n",
            "('of', 'study'): 1\n",
            "('study', '.'): 1\n",
            "\n",
            "=== TRIGRAMS ===\n",
            "\n",
            "First 10 trigrams: [('natural', 'language', 'processing'), ('language', 'processing', 'is'), ('processing', 'is', 'an'), ('is', 'an', 'exciting'), ('an', 'exciting', 'field'), ('exciting', 'field', 'of'), ('field', 'of', 'study'), ('of', 'study', '.')]\n",
            "\n",
            "Top 10 most common trigrams:\n",
            "('natural', 'language', 'processing'): 1\n",
            "('language', 'processing', 'is'): 1\n",
            "('processing', 'is', 'an'): 1\n",
            "('is', 'an', 'exciting'): 1\n",
            "('an', 'exciting', 'field'): 1\n",
            "('exciting', 'field', 'of'): 1\n",
            "('field', 'of', 'study'): 1\n",
            "('of', 'study', '.'): 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"\"\"Natural language processing is a field of artificial intelligence.\n",
        "It deals with the interaction between computers and humans using natural language.\n",
        "Processing includes tasks such as tokenization, parsing, and sentiment analysis.\n",
        "Understanding language is crucial for applications like chatbots, translation, and information retrieval.\n",
        "Language processing helps in deriving meaning from text and is an essential part of modern AI systems.\"\"\"\n",
        "\n",
        "def most_probable_next_word(text, w1):\n",
        "    words = word_tokenize(text)\n",
        "    bigrams = Counter(nltk.bigrams(words))\n",
        "    following_words = {w2: count for (prev, w2), count in bigrams.items() if prev == w1}\n",
        "    return max(following_words, key=following_words.get, default=None), sum(following_words.values())\n",
        "\n",
        "w1 = input(\"Enter a word: \")\n",
        "w2, count = most_probable_next_word(text, w1)\n",
        "\n",
        "if w2:\n",
        "    print(f\"'{w2}' is most likely to follow '{w1}' with frequency {count}.\")\n",
        "else:\n",
        "    print(f\"No words found after '{w1}'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsb8mlDUOK3V",
        "outputId": "1d4159e6-c09a-44a9-bc5d-1d3e6c244c1e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a word: language\n",
            "'processing' is most likely to follow 'language' with frequency 3.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def function(text,w1) :\n",
        "  following_words = {}\n",
        "  words = word_tokenize(text)\n",
        "  bigrams = Counter(nltk.bigrams(words))\n",
        "  for (prev, w2), count in bigrams.items():\n",
        "    if prev == w1 :\n",
        "      following_words[w2] = count\n",
        "  most_freq_word = None\n",
        "  max_count = -1\n",
        "  for w,c in following_words.items():\n",
        "    if c > max_count :\n",
        "      max_count = c\n",
        "      most_freq_word = w\n",
        "  most_freq = sum(following_words.values())\n",
        "  return most_freq_word,most_freq"
      ],
      "metadata": {
        "id": "eFAEwfK0_97k"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Week 4"
      ],
      "metadata": {
        "id": "EsXRESrcS121"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.collocations import BigramAssocMeasures\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample text\n",
        "text = \"Machine learning is a fascinating field of artificial intelligence. \\\n",
        "It allows computers to learn from data and make predictions. \\\n",
        "Deep learning, a subset of machine learning, focuses on neural networks.\"\n",
        "\n",
        "# Tokenization\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "# Finding bigram collocations\n",
        "bigram_measures = BigramAssocMeasures()\n",
        "finder = BigramCollocationFinder.from_words(words)\n",
        "\n",
        "# Get top 5 collocations based on PMI\n",
        "collocations = finder.nbest(bigram_measures.pmi, 5)\n",
        "print(collocations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFcr_6WHS1ii",
        "outputId": "f46a4499-aa8b-4554-c475-64af21bbfe46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('allows', 'computers'), ('artificial', 'intelligence'), ('computers', 'learn'), ('data', 'make'), ('fascinating', 'field')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "words_with_prefix = []\n",
        "\n",
        "# Function to find words beginning with a given prefix\n",
        "def find_words_starting_with(text, prefix):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text.lower())  # Convert text to lowercase for case-insensitive matching\n",
        "\n",
        "    # Filter words starting with the given prefix\n",
        "    for word in words :\n",
        "      if word.startswith(prefix.lower()):\n",
        "        words_with_prefix.append(word)\n",
        "\n",
        "    return words_with_prefix\n",
        "\n",
        "# Example: Find all words starting with 'na'\n",
        "prefix = input(\"enter prefix:\")\n",
        "words_with_prefix = find_words_starting_with(text, prefix)\n",
        "\n",
        "# Print the words\n",
        "print(f\"Words starting with '{prefix}':\")\n",
        "print(words_with_prefix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS8h77xLUfEW",
        "outputId": "8f6c4171-c089-417e-f273-7d47fd92ca9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter prefix:mac\n",
            "Words starting with 'mac':\n",
            "['machine', 'machine']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "# text = \"\"\"\n",
        "\"Natural Language Processing (NLP) is an important field of artificial intelligence. NLP techniques are used to process human languages for various applications . Machine learning and deep learning models have greatly improved NLP capabilities, making it a powerful tool.\"\n",
        "# \"\"\"\n",
        "#text=input(\"enter text:\")\n",
        "def long_words(text, min_length=4):\n",
        "    words = word_tokenize(text)\n",
        "    lon_words = [word for word in words if len(word) > min_length]\n",
        "    return lon_words\n",
        "words = long_words(text)\n",
        "print(f\"Words longer than four characters:\")\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZEFCADtVA5r",
        "outputId": "9bb708dd-bbb0-45c7-add6-86ea8d4bf8ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words longer than four characters:\n",
            "['Natural', 'language', 'processing', 'field', 'artificial', 'intelligence', 'deals', 'interaction', 'between', 'computers', 'humans', 'using', 'natural', 'language', 'Processing', 'includes', 'tasks', 'tokenization', 'parsing', 'sentiment', 'analysis', 'Understanding', 'language', 'crucial', 'applications', 'chatbots', 'translation', 'information', 'retrieval', 'Language', 'processing', 'helps', 'deriving', 'meaning', 'essential', 'modern', 'systems']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Week5"
      ],
      "metadata": {
        "id": "6uDj3-D1VOn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def find_math_expressions(sentence):\n",
        "    # Refined regular expression to match mathematical expressions\n",
        "    math_expression_pattern = r'[A-Za-z\\d]+(?:\\s*[\\+\\-\\/\\^\\=]\\s[A-Za-z\\d]+)+'\n",
        "\n",
        "    # Find all matches in the sentence\n",
        "    math_expressions = re.findall(math_expression_pattern, sentence)\n",
        "\n",
        "    return math_expressions\n",
        "\n",
        "# Example input\n",
        "sentence = input(\"Enter a sentence: \")\n",
        "\n",
        "# Identify mathematical expressions\n",
        "math_expressions = find_math_expressions(sentence)\n",
        "\n",
        "if math_expressions:\n",
        "    print(\"Mathematical expressions found:\", math_expressions)\n",
        "else:\n",
        "    print(\"No mathematical expressions found.\")\n",
        "#Enter a sentence: The area of a circle is given by the formula A = pi * r^2. Also, 3 + 5 = 8 is true."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm8ycXe2VMpK",
        "outputId": "19f0bef1-f791-4917-ab9b-b69461fda4ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: The area of a circle is given by the formula A = pi * r^2. Also, 3 + 5 = 8 is true\n",
            "Mathematical expressions found: ['A = pi', '3 + 5 = 8']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_email_components(email):\n",
        "    # Regular expression to match an email address\n",
        "    email_pattern = r'^([a-zA-Z0-9._%+-]+)@([a-zA-Z0-9.-]+)\\.([a-zA-Z]{2,})$'\n",
        "\n",
        "    match = re.match(email_pattern, email)\n",
        "    if match:\n",
        "        local_part = match.group(1)\n",
        "        domain = match.group(2)\n",
        "        top_level_domain = match.group(3)\n",
        "        return local_part, domain, top_level_domain\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Input from the user\n",
        "email = input(\"Enter an email address: \")\n",
        "\n",
        "# Extract components\n",
        "components = extract_email_components(email)\n",
        "\n",
        "if components:\n",
        "    print(f\"Local part: {components[0]}\")\n",
        "    print(f\"Domain: {components[1]}\")\n",
        "    print(f\"Top-level domain: {components[2]}\")\n",
        "else:\n",
        "    print(\"Invalid email address format.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LKzTbU0WS5P",
        "outputId": "f1d37a78-b5ea-4052-8ac1-9098ebe80006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter an email address: 322103382058@gvpce.ac.in\n",
            "Local part: 322103382058\n",
            "Domain: gvpce.ac\n",
            "Top-level domain: in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Week 6"
      ],
      "metadata": {
        "id": "_xNfzQz6W5b3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def get_synonyms_antonyms(word):\n",
        "    # Get synsets (synonym sets) for the word\n",
        "    synsets = wn.synsets(word)\n",
        "\n",
        "    synonyms = set()\n",
        "    antonyms = set()\n",
        "\n",
        "    for synset in synsets:\n",
        "        # Add synonyms to the set\n",
        "        for lemma in synset.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "\n",
        "            # Add antonyms to the set\n",
        "            if lemma.antonyms():\n",
        "                antonyms.add(lemma.antonyms()[0].name())\n",
        "\n",
        "    return list(synonyms), list(antonyms)\n",
        "\n",
        "# Input word from user\n",
        "word = input(\"Enter a word: \")\n",
        "\n",
        "# Get synonyms and antonyms\n",
        "synonyms, antonyms = get_synonyms_antonyms(word)\n",
        "\n",
        "if synonyms:\n",
        "    print(f\"Synonyms of {word}: {', '.join(synonyms)}\")\n",
        "else:\n",
        "    print(f\"No synonyms found for {word}.\")\n",
        "\n",
        "if antonyms:\n",
        "    print(f\"Antonyms of {word}: {', '.join(antonyms)}\")\n",
        "else:\n",
        "    print(f\"No antonyms found for {word}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA37lAIxW73g",
        "outputId": "ba6da6f6-17b2-4d35-e730-39e62ae0dc6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a word: natural\n",
            "Synonyms of natural: cancel, instinctive, lifelike, rude, natural, raw, innate, born\n",
            "Antonyms of natural: supernatural, sharp, artificial, unnatural\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def get_hyponyms(word):\n",
        "    synsets = wn.synsets(word)\n",
        "    hyponyms = set()\n",
        "    for synset in synsets:\n",
        "        for hyponym in synset.hyponyms():\n",
        "            hyponyms.add(hyponym.name().split('.')[0])  # Get the word part of the hyponym\n",
        "    return list(hyponyms)\n",
        "\n",
        "def get_homonyms(word):\n",
        "    synsets = wn.synsets(word)\n",
        "    homonyms = set()\n",
        "    for synset in synsets:\n",
        "        # Collect homonyms that have multiple meanings (different synsets)\n",
        "        homonyms.add(synset.name().split('.')[0])  # Get the word part of the homonym\n",
        "    return list(homonyms)\n",
        "\n",
        "def get_polysemy(word):\n",
        "    synsets = wn.synsets(word)\n",
        "    return len(synsets)\n",
        "\n",
        "# Main function to execute the program\n",
        "def main():\n",
        "    word = input(\"Enter a word: \")\n",
        "\n",
        "    # Get hyponyms\n",
        "    hyponyms = get_hyponyms(word)\n",
        "    if hyponyms:\n",
        "        print(f\"Hyponyms of {word}: {', '.join(hyponyms)}\")\n",
        "    else:\n",
        "        print(f\"No hyponyms found for {word}.\")\n",
        "\n",
        "    # Get homonyms\n",
        "    homonyms = get_homonyms(word)\n",
        "    if len(homonyms) > 1:\n",
        "        print(f\"Homonyms of {word}: {', '.join(homonyms)}\")\n",
        "    else:\n",
        "        print(f\"No homonyms found for {word}.\")\n",
        "\n",
        "    # Get polysemy count\n",
        "    polysemy_count = get_polysemy(word)\n",
        "    print(f\"{word} has {polysemy_count} meanings (Polysemy count).\")\n",
        "\n",
        "# Call the main function directly\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9QQCMeCYxVb",
        "outputId": "519fa1dd-4dfd-43e3-c39c-5d766f0df2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a word: is\n",
            "Hyponyms of is: begin, cox, stand, connect, incarnate, press, trim, distribute, deserve, rage, attend, jumble, iridesce, osculate, follow, be_well, coexist, form, buy, present, stink, fall, extend, head, confuse, flow, cost, clean, seethe, retard, supplement, stick, kill, lie, vet, mope, promise, stretch, belong, look, put_out, prove, impend, rate, rank, inhabit, sell, reach, consist, total, pay, cover, object, prevail, suit, compact, sit, come_in_handy, want, compare, sparkle, center_on, straddle, contain, bake, deck, owe, run_into, transplant, go, beat, equate, cut_across, loiter, compose, answer, suck, squat, underlie, end, tend, figure, recognize, dwell, feel, range, let_go, rut, cut, represent, breathe, point, test, preexist, litter, relate, kick_around, populate, fit, face, hoodoo, exemplify, come_in_for, set_back, disagree, stagnate, hail, hang, specify, account_for, suffer, endanger, remain, body, act, seem, work, match, stand_by, swing, wind, subtend, account, hold, stay, rest, hum, wash, lend, continue, run, draw, measure, fall_into, make, cohere, diverge, gravitate, balance, shine, swim, depend, lubricate, translate, accept, appear, gape, indwell, abound, occupy, make_sense, count, stand_back, come\n",
            "Homonyms of is: cost, constitute, exist, equal, be, embody\n",
            "is has 13 meanings (Polysemy count).\n"
          ]
        }
      ]
    }
  ]
}