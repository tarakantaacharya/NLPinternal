{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Week 2\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uWfxIEkfLhEB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7W_di--fLfPH",
        "outputId": "65c667db-306d-41cc-a6d6-49a5a2c114b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paragraphs:\n",
            " ['Natural Language Processing (NLP) helps computers understand human language.', 'Tokenization breaks text into words and sentences for analysis.']\n",
            "Paragraph 1: Natural Language Processing (NLP) helps computers understand human language.\n",
            "\n",
            "Paragraph 2: Tokenization breaks text into words and sentences for analysis.\n",
            "\n",
            "Sentences:\n",
            " ['Natural Language Processing (NLP) helps computers understand human language.', 'Tokenization breaks text into words and sentences for analysis.']\n",
            "Sentence 1: Natural Language Processing (NLP) helps computers understand human language.\n",
            "Sentence 2: Tokenization breaks text into words and sentences for analysis.\n",
            "\n",
            "Words:\n",
            " ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'helps', 'computers', 'understand', 'human', 'language', '.', 'Tokenization', 'breaks', 'text', 'into', 'words', 'and', 'sentences', 'for', 'analysis', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')  # Required for tokenizers\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Sample text with multiple paragraphs\n",
        "# text=input(\"enter text:\\n\")\n",
        "text = '''Natural Language Processing (NLP) helps computers understand human language.\n",
        "\n",
        "Tokenization breaks text into words and sentences for analysis.'''\n",
        "\n",
        "# Word Tokenizer\n",
        "def word_tokenizer(text):\n",
        "    words = word_tokenize(text)\n",
        "    return words\n",
        "\n",
        "# Sentence Tokenizer\n",
        "def sentence_tokenizer(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    return sentences\n",
        "\n",
        "# Paragraph Tokenizer\n",
        "def paragraph_tokenizer(text):\n",
        "    paragraphs = text.split(\"\\n\\n\")  # Splitting based on double newline characters\n",
        "    return paragraphs\n",
        "\n",
        "# Using the tokenizers\n",
        "# print(\"Original Text:\")\n",
        "# print(text, \"\\n\")\n",
        "\n",
        "# Paragraph Tokenization\n",
        "paragraphs = paragraph_tokenizer(text)\n",
        "print(\"Paragraphs:\\n\",paragraphs)\n",
        "for i, para in enumerate(paragraphs, 1):\n",
        "    print(f\"Paragraph {i}: {para}\\n\")\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentences = sentence_tokenizer(text)\n",
        "print(\"Sentences:\\n\",sentences)\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"Sentence {i}: {sentence}\")\n",
        "\n",
        "# Word Tokenization\n",
        "words = word_tokenizer(text)\n",
        "print(\"\\nWords:\\n\",words)\n",
        "# print(words)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('brown')  # Download Brown corpus if not already downloaded\n",
        "\n",
        "# Select a corpus (example: Brown corpus)\n",
        "corpus_words = brown.words()\n",
        "\n",
        "# Calculate total words and unique words\n",
        "total_words = len(corpus_words)  # Total number of words\n",
        "distinct_words = len(set(corpus_words))  # Total unique words (using set)\n",
        "\n",
        "# Print results\n",
        "print(f\"Total words in the corpus: {total_words}\")\n",
        "print(f\"Number of distinct words in the corpus: {distinct_words}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQamWzRmLnZV",
        "outputId": "173ff57d-5871-47f5-ae8d-dac26b5c21c9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words in the corpus: 1161192\n",
            "Number of distinct words in the corpus: 56057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Week3"
      ],
      "metadata": {
        "id": "PVW76C-qMs7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "def text_preprocessing(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize into words\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "def generate_unigrams(text):\n",
        "    tokens = text_preprocessing(text)\n",
        "    # Generate unigrams\n",
        "    unigrams = list(ngrams(tokens, 1))\n",
        "    # Count frequencies\n",
        "    unigram_freq = Counter(unigrams)\n",
        "    return unigrams, unigram_freq\n",
        "\n",
        "print(\"=== UNIGRAMS ===\")\n",
        "unigrams, unigram_freq = generate_unigrams(text)\n",
        "print(\"\\nFirst 10 unigrams:\", unigrams[:10])\n",
        "print(\"\\nTop 10 most common unigrams:\")\n",
        "for gram, count in unigram_freq.most_common(10):\n",
        "    print(f\"{gram}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkGFTPGiMsnR",
        "outputId": "f6f4daa6-fbcf-40a0-9a81-e44ac2f92f35"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== UNIGRAMS ===\n",
            "\n",
            "First 10 unigrams: [('natural',), ('language',), ('processing',), ('(',), ('nlp',), (')',), ('helps',), ('computers',), ('understand',), ('human',)]\n",
            "\n",
            "Top 10 most common unigrams:\n",
            "('language',): 2\n",
            "('.',): 2\n",
            "('natural',): 1\n",
            "('processing',): 1\n",
            "('(',): 1\n",
            "('nlp',): 1\n",
            "(')',): 1\n",
            "('helps',): 1\n",
            "('computers',): 1\n",
            "('understand',): 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"\"\"Natural language processing is a field of artificial intelligence.\n",
        "It deals with the interaction between computers and humans using natural language.\n",
        "Processing includes tasks such as tokenization, parsing, and sentiment analysis.\n",
        "Understanding language is crucial for applications like chatbots, translation, and information retrieval.\n",
        "Language processing helps in deriving meaning from text and is an essential part of modern AI systems.\"\"\"\n",
        "\n",
        "def most_probable_next_word(text, w1):\n",
        "    words = word_tokenize(text)\n",
        "    bigrams = list(nltk.bigrams(words))\n",
        "    bigram_counts = Counter(bigrams)\n",
        "\n",
        "    # Find the most frequent word appearing after w1\n",
        "    following_words = {}  # Create an empty dictionary\n",
        "\n",
        "    # Loop through each bigram (prev, w2) and its count\n",
        "    for bigram, count in bigram_counts.items():\n",
        "        prev, w2 = bigram  # Extract the two words in the bigram\n",
        "\n",
        "        # Check if the first word in the bigram matches w1\n",
        "        if prev == w1:\n",
        "            following_words[w2] = count  # Store the second word and its count\n",
        "\n",
        "    if following_words:\n",
        "        w2 = max(following_words, key=following_words.get)\n",
        "        return w2, following_words[w2] / sum(following_words.values())\n",
        "    else:\n",
        "        return None, 0.0\n",
        "\n",
        "# User input\n",
        "w1 = input(\"Enter a word: \")\n",
        "w2, prob = most_probable_next_word(text, w1)\n",
        "\n",
        "if w2:\n",
        "    print(f\"'{w2}' is most likely to follow '{w1}' with probability {prob:.2f}.\")\n",
        "else:\n",
        "    print(f\"No words found after '{w1}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsb8mlDUOK3V",
        "outputId": "bfbe39d3-4175-4e03-b3a5-338864501a0c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a word: a\n",
            "'field' is most likely to follow 'a' with probability 1.00.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Week 4"
      ],
      "metadata": {
        "id": "EsXRESrcS121"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "from nltk.metrics import BigramAssocMeasures\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text.lower())  # Convert to lowercase to treat words uniformly\n",
        "\n",
        "# Remove stopwords (common words that might not be useful for finding collocations)\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [word for word in tokens if word not in stop_words and word.isalpha()]  # Keep only non-stop words and alphanumeric tokens\n",
        "\n",
        "# Find bigrams using BigramCollocationFinder\n",
        "bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
        "\n",
        "# Use association measures to rank bigrams by likelihood ratio or PMI (Pointwise Mutual Information)\n",
        "collocations = bigram_finder.nbest(BigramAssocMeasures.likelihood_ratio, 5)\n",
        "\n",
        "# Print the top 5 collocations\n",
        "print(\"Top 5 collocations:\")\n",
        "for collocation in collocations:\n",
        "    print(\" \".join(collocation))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFcr_6WHS1ii",
        "outputId": "007683fb-9d50-4511-ace4-787173b8c1bb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 collocations:\n",
            "language processing\n",
            "natural language\n",
            "ai systems\n",
            "analysis understanding\n",
            "applications like\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Function to find words beginning with a given prefix\n",
        "def find_words_starting_with(text, prefix):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text.lower())  # Convert text to lowercase for case-insensitive matching\n",
        "\n",
        "    # Filter words starting with the given prefix\n",
        "    words_with_prefix = [word for word in words if word.startswith(prefix.lower())]\n",
        "\n",
        "    return words_with_prefix\n",
        "\n",
        "# Example: Find all words starting with 'na'\n",
        "prefix = input(\"enter prefix:\")\n",
        "words_with_prefix = find_words_starting_with(text, prefix)\n",
        "\n",
        "# Print the words\n",
        "print(f\"Words starting with '{prefix}':\")\n",
        "print(words_with_prefix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS8h77xLUfEW",
        "outputId": "2c10b458-c570-4059-dd27-f9b0cd83f11b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter prefix:Nat\n",
            "Words starting with 'Nat':\n",
            "['natural', 'natural']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "# text = \"\"\"\n",
        "\"Natural Language Processing (NLP) is an important field of artificial intelligence. NLP techniques are used to process human languages for various applications . Machine learning and deep learning models have greatly improved NLP capabilities, making it a powerful tool.\"\n",
        "# \"\"\"\n",
        "#text=input(\"enter text:\")\n",
        "def long_words(text, min_length=4):\n",
        "    words = word_tokenize(text)\n",
        "    lon_words = [word for word in words if len(word) > min_length]\n",
        "    return lon_words\n",
        "words = long_words(text)\n",
        "print(f\"Words longer than four characters:\")\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZEFCADtVA5r",
        "outputId": "9bb708dd-bbb0-45c7-add6-86ea8d4bf8ff"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words longer than four characters:\n",
            "['Natural', 'language', 'processing', 'field', 'artificial', 'intelligence', 'deals', 'interaction', 'between', 'computers', 'humans', 'using', 'natural', 'language', 'Processing', 'includes', 'tasks', 'tokenization', 'parsing', 'sentiment', 'analysis', 'Understanding', 'language', 'crucial', 'applications', 'chatbots', 'translation', 'information', 'retrieval', 'Language', 'processing', 'helps', 'deriving', 'meaning', 'essential', 'modern', 'systems']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Week5"
      ],
      "metadata": {
        "id": "6uDj3-D1VOn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def find_math_expressions(sentence):\n",
        "    # Refined regular expression to match mathematical expressions\n",
        "    math_expression_pattern = r'[A-Za-z\\d]+(?:\\s*[\\+\\-\\/\\^\\=]\\s[A-Za-z\\d]+)+'\n",
        "\n",
        "    # Find all matches in the sentence\n",
        "    math_expressions = re.findall(math_expression_pattern, sentence)\n",
        "\n",
        "    return math_expressions\n",
        "\n",
        "# Example input\n",
        "sentence = input(\"Enter a sentence: \")\n",
        "\n",
        "# Identify mathematical expressions\n",
        "math_expressions = find_math_expressions(sentence)\n",
        "\n",
        "if math_expressions:\n",
        "    print(\"Mathematical expressions found:\", math_expressions)\n",
        "else:\n",
        "    print(\"No mathematical expressions found.\")\n",
        "#Enter a sentence: The area of a circle is given by the formula A = pi * r^2. Also, 3 + 5 = 8 is true."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm8ycXe2VMpK",
        "outputId": "19f0bef1-f791-4917-ab9b-b69461fda4ae"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: The area of a circle is given by the formula A = pi * r^2. Also, 3 + 5 = 8 is true\n",
            "Mathematical expressions found: ['A = pi', '3 + 5 = 8']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_email_components(email):\n",
        "    # Regular expression to match an email address\n",
        "    email_pattern = r'^([a-zA-Z0-9._%+-]+)@([a-zA-Z0-9.-]+)\\.([a-zA-Z]{2,})$'\n",
        "\n",
        "    match = re.match(email_pattern, email)\n",
        "    if match:\n",
        "        local_part = match.group(1)\n",
        "        domain = match.group(2)\n",
        "        top_level_domain = match.group(3)\n",
        "        return local_part, domain, top_level_domain\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Input from the user\n",
        "email = input(\"Enter an email address: \")\n",
        "\n",
        "# Extract components\n",
        "components = extract_email_components(email)\n",
        "\n",
        "if components:\n",
        "    print(f\"Local part: {components[0]}\")\n",
        "    print(f\"Domain: {components[1]}\")\n",
        "    print(f\"Top-level domain: {components[2]}\")\n",
        "else:\n",
        "    print(\"Invalid email address format.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LKzTbU0WS5P",
        "outputId": "cf7ec293-978e-491a-bfc5-3240a5f900de"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter an email address: acharyatarakanta2002@gmail.com\n",
            "Local part: acharyatarakanta2002\n",
            "Domain: gmail\n",
            "Top-level domain: com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Week 6"
      ],
      "metadata": {
        "id": "_xNfzQz6W5b3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def get_synonyms_antonyms(word):\n",
        "    # Get synsets (synonym sets) for the word\n",
        "    synsets = wn.synsets(word)\n",
        "\n",
        "    synonyms = set()\n",
        "    antonyms = set()\n",
        "\n",
        "    for synset in synsets:\n",
        "        # Add synonyms to the set\n",
        "        for lemma in synset.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "\n",
        "            # Add antonyms to the set\n",
        "            if lemma.antonyms():\n",
        "                antonyms.add(lemma.antonyms()[0].name())\n",
        "\n",
        "    return list(synonyms), list(antonyms)\n",
        "\n",
        "# Input word from user\n",
        "word = input(\"Enter a word: \")\n",
        "\n",
        "# Get synonyms and antonyms\n",
        "synonyms, antonyms = get_synonyms_antonyms(word)\n",
        "\n",
        "if synonyms:\n",
        "    print(f\"Synonyms of {word}: {', '.join(synonyms)}\")\n",
        "else:\n",
        "    print(f\"No synonyms found for {word}.\")\n",
        "\n",
        "if antonyms:\n",
        "    print(f\"Antonyms of {word}: {', '.join(antonyms)}\")\n",
        "else:\n",
        "    print(f\"No antonyms found for {word}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA37lAIxW73g",
        "outputId": "ba6da6f6-17b2-4d35-e730-39e62ae0dc6e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a word: natural\n",
            "Synonyms of natural: cancel, instinctive, lifelike, rude, natural, raw, innate, born\n",
            "Antonyms of natural: supernatural, sharp, artificial, unnatural\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def get_hyponyms(word):\n",
        "    synsets = wn.synsets(word)\n",
        "    hyponyms = set()\n",
        "    for synset in synsets:\n",
        "        for hyponym in synset.hyponyms():\n",
        "            hyponyms.add(hyponym.name().split('.')[0])  # Get the word part of the hyponym\n",
        "    return list(hyponyms)\n",
        "\n",
        "def get_homonyms(word):\n",
        "    synsets = wn.synsets(word)\n",
        "    homonyms = set()\n",
        "    for synset in synsets:\n",
        "        # Collect homonyms that have multiple meanings (different synsets)\n",
        "        homonyms.add(synset.name().split('.')[0])  # Get the word part of the homonym\n",
        "    return list(homonyms)\n",
        "\n",
        "def get_polysemy(word):\n",
        "    synsets = wn.synsets(word)\n",
        "    return len(synsets)\n",
        "\n",
        "# Main function to execute the program\n",
        "def main():\n",
        "    word = input(\"Enter a word: \")\n",
        "\n",
        "    # Get hyponyms\n",
        "    hyponyms = get_hyponyms(word)\n",
        "    if hyponyms:\n",
        "        print(f\"Hyponyms of {word}: {', '.join(hyponyms)}\")\n",
        "    else:\n",
        "        print(f\"No hyponyms found for {word}.\")\n",
        "\n",
        "    # Get homonyms\n",
        "    homonyms = get_homonyms(word)\n",
        "    if len(homonyms) > 1:\n",
        "        print(f\"Homonyms of {word}: {', '.join(homonyms)}\")\n",
        "    else:\n",
        "        print(f\"No homonyms found for {word}.\")\n",
        "\n",
        "    # Get polysemy count\n",
        "    polysemy_count = get_polysemy(word)\n",
        "    print(f\"{word} has {polysemy_count} meanings (Polysemy count).\")\n",
        "\n",
        "# Call the main function directly\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9QQCMeCYxVb",
        "outputId": "519fa1dd-4dfd-43e3-c39c-5d766f0df2f2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a word: is\n",
            "Hyponyms of is: begin, cox, stand, connect, incarnate, press, trim, distribute, deserve, rage, attend, jumble, iridesce, osculate, follow, be_well, coexist, form, buy, present, stink, fall, extend, head, confuse, flow, cost, clean, seethe, retard, supplement, stick, kill, lie, vet, mope, promise, stretch, belong, look, put_out, prove, impend, rate, rank, inhabit, sell, reach, consist, total, pay, cover, object, prevail, suit, compact, sit, come_in_handy, want, compare, sparkle, center_on, straddle, contain, bake, deck, owe, run_into, transplant, go, beat, equate, cut_across, loiter, compose, answer, suck, squat, underlie, end, tend, figure, recognize, dwell, feel, range, let_go, rut, cut, represent, breathe, point, test, preexist, litter, relate, kick_around, populate, fit, face, hoodoo, exemplify, come_in_for, set_back, disagree, stagnate, hail, hang, specify, account_for, suffer, endanger, remain, body, act, seem, work, match, stand_by, swing, wind, subtend, account, hold, stay, rest, hum, wash, lend, continue, run, draw, measure, fall_into, make, cohere, diverge, gravitate, balance, shine, swim, depend, lubricate, translate, accept, appear, gape, indwell, abound, occupy, make_sense, count, stand_back, come\n",
            "Homonyms of is: cost, constitute, exist, equal, be, embody\n",
            "is has 13 meanings (Polysemy count).\n"
          ]
        }
      ]
    }
  ]
}